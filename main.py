"""
MCP Server para an√°lisis de calidad de software
Punto de entrada centralizado para todas las herramientas de QA
"""
from fastmcp import FastMCP
from tools.swagger_analyzer import SwaggerAnalyzerTool
from tools.test_case_generator import TestCaseGeneratorTool

# Inicializar MCP
mcp = FastMCP('Calidad de Software MCP')

# Inicializar herramientas
swagger_tool = SwaggerAnalyzerTool()
test_case_generator_tool = TestCaseGeneratorTool()


@mcp.tool()
def analizar_contrato_swagger(
    url: str,
    swagger_ui_url: str = None,
    generar_json: bool = True,
    generar_readme: bool = True
) -> str:
    """
    Analiza un contrato Swagger/OpenAPI y genera an√°lisis completo con exportaciones.
    
    Esta herramienta realiza un an√°lisis exhaustivo del contrato y genera:
    - An√°lisis detallado en formato texto
    - Archivo JSON con toda la informaci√≥n estructurada (opcional)
    - README con documentaci√≥n estilo Swagger UI (opcional)
    
    Extrae informaci√≥n completa incluyendo:
    - Endpoints (paths, m√©todos HTTP, par√°metros)
    - Request bodies (schemas, content types, validaciones)
    - Responses (c√≥digos HTTP, schemas, headers)
    - Schemas/Definiciones (propiedades, tipos, formatos)
    - Validaciones (obligatoriedad, min/max, patterns, enums)
    - Informaci√≥n de servidores y seguridad
    - Tags y documentaci√≥n externa
    
    Los archivos se guardan en: output/swagger_analyzer/
    
    Args:
        url: URL del contrato Swagger/OpenAPI (JSON o YAML)
        swagger_ui_url: URL de Swagger UI para incluir en el README (opcional)
        generar_json: Generar archivo JSON con an√°lisis completo (por defecto: True)
        generar_readme: Generar README con documentaci√≥n (por defecto: True)
        
    Returns:
        An√°lisis detallado y rutas de archivos generados
        
    Examples:
        # An√°lisis completo (texto + JSON + README)
        analizar_contrato_swagger("http://localhost:8080/v3/api-docs")
        
        # Con URL de Swagger UI
        analizar_contrato_swagger(
            "http://localhost:8080/v3/api-docs",
            "http://localhost:8080/swagger-ui/index.html"
        )
        
        # Solo an√°lisis de texto, sin generar archivos
        analizar_contrato_swagger(
            "https://petstore.swagger.io/v2/swagger.json",
            generar_json=False,
            generar_readme=False
        )
    """
    return swagger_tool.analyze_contract(
        url=url,
        swagger_ui_url=swagger_ui_url,
        generate_json=generar_json,
        generate_readme=generar_readme
    )


@mcp.tool()
def generar_casos_prueba(
    swagger_analysis_json_path: str,
    tecnicas: list[str] = None,
    incluir_positivos: bool = True,
    incluir_negativos: bool = True,
    generar_json: bool = True,
    generar_readme: bool = True
) -> str:
    """
    Genera casos de prueba autom√°ticamente desde un an√°lisis de Swagger usando t√©cnicas ISTQB.
    
    Esta herramienta crea casos de prueba completos aplicando t√©cnicas de testing reconocidas:
    - Partici√≥n de Equivalencia (equivalence_partitioning)
    - An√°lisis de Valores L√≠mite (boundary_value_analysis)
    - Tabla de Decisi√≥n (decision_table)
    
    Genera casos detallados con:
    - ID √∫nico y nombre descriptivo
    - T√©cnica ISTQB aplicada (trazabilidad)
    - Tipo (positivo/negativo)
    - Prioridad (alta/media/baja)
    - Datos de entrada sint√©ticos realistas (headers, params, body)
    - Resultado esperado (c√≥digo HTTP, errores espec√≠ficos)
    - Pre y post condiciones
    
    Los archivos se guardan en: output/test_case_generator/
    
    Args:
        swagger_analysis_json_path: Ruta al JSON generado por analizar_contrato_swagger
        tecnicas: Lista de t√©cnicas ISTQB a aplicar (default: todas)
                 Valores: ["equivalence_partitioning", "boundary_value_analysis", "decision_table"]
        incluir_positivos: Incluir casos de prueba positivos (default: True)
        incluir_negativos: Incluir casos de prueba negativos (default: True)
        generar_json: Generar archivo JSON con casos de prueba (default: True)
        generar_readme: Generar README con listado de casos (default: True)
        
    Returns:
        Resumen de casos generados y rutas de archivos
        
    Examples:
        # Generar todos los casos con todas las t√©cnicas
        generar_casos_prueba("output/swagger_analyzer/swagger-analysis.json")
        
        # Solo casos de valores l√≠mite
        generar_casos_prueba(
            "output/swagger_analyzer/swagger-analysis.json",
            tecnicas=["boundary_value_analysis"]
        )
        
        # Solo casos negativos
        generar_casos_prueba(
            "output/swagger_analyzer/swagger-analysis.json",
            incluir_positivos=False,
            incluir_negativos=True
        )
    """
    result = test_case_generator_tool.generate_test_cases(
        swagger_analysis_json_path=swagger_analysis_json_path,
        techniques=tecnicas,
        include_positive=incluir_positivos,
        include_negative=incluir_negativos,
        generate_json=generar_json,
        generate_readme=generar_readme
    )
    
    # Formatear resultado
    output = []
    output.append("‚úÖ Generaci√≥n de casos de prueba completada\n")
    output.append(f"üìä Total de casos generados: {result['total_test_cases']}\n")
    
    summary = result['summary']
    
    output.append("üéØ Distribuci√≥n por t√©cnica:")
    for technique, count in summary['by_technique'].items():
        output.append(f"  - {technique}: {count} casos")
    
    output.append("\nüìã Distribuci√≥n por tipo:")
    for test_type, count in summary['by_type'].items():
        icon = "‚úÖ" if test_type == "positive" else "‚ùå"
        output.append(f"  {icon} {test_type}: {count} casos")
    
    output.append("\n‚ö° Distribuci√≥n por prioridad:")
    for priority, count in summary['by_priority'].items():
        icon = "üî¥" if priority == "high" else "üü°" if priority == "medium" else "üü¢"
        output.append(f"  {icon} {priority}: {count} casos")
    
    output.append("\nüìÅ Archivos generados:")
    for file_info in result['files_generated']:
        output.append(f"  {file_info['type']}: {file_info['path']}")
    
    return "\n".join(output)


if __name__ == "__main__":
    mcp.run()

